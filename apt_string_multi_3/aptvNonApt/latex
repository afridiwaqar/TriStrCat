\begin{table}[H]
    \centering
    \begin{adjustbox}{max width=\textwidth}
        \begin{tabular}{@{}lcccc@{}}
            \toprule
            \textbf{Model} & \textbf{Accuracy (Mean ± Std)} & \textbf{Precision (Mean ± Std)} & \textbf{Recall (Mean ± Std)} & \textbf{F1 Score (Mean ± Std)} \\ \midrule
            \textbf{Logistic Regression} & 0.9242 ± 0.0104 & 0.9192 ± 0.0074 & 0.9295 ± 0.0159 & 0.9246 ± 0.0107 \\
            \textbf{Random Forest} & 0.9370 ± 0.0058 & 0.9208 ± 0.0063 & 0.9582 ± 0.0058 & 0.9380 ± 0.0074 \\
            \textbf{Gradient Boosting} & 0.9448 ± 0.0106 & 0.9413 ± 0.0111 & 0.9483 ± 0.0119 & 0.9441 ± 0.0100 \\
            \textbf{k-NN} & 0.9152 ± 0.0077 & 0.9128 ± 0.0094 & 0.9183 ± 0.0156 & 0.9154 ± 0.0081 \\
            \textbf{Gaussian Naive Bayes} & 0.8971 ± 0.0083 & 0.9477 ± 0.0171 & 0.8410 ± 0.0076 & 0.8911 ± 0.0081 \\
            \textbf{SVC} & 0.9277 ± 0.0089 & 0.9240 ± 0.0014 & 0.9320 ± 0.0177 & 0.9279 ± 0.0094 \\
            \textbf{AdaBoost} & 0.9314 ± 0.0049 & 0.9246 ± 0.0066 & 0.9395 ± 0.0038 & 0.9320 ± 0.0047 \\
            \textbf{Linear Discriminant Analysis} & 0.7347 ± 0.0412 & 0.7268 ± 0.0262 & 0.7487 ± 0.0718 & 0.7369 ± 0.0481 \\
            \textbf{Quadratic Discriminant Analysis} & 0.8974 ± 0.0034 & 0.8602 ± 0.0089 & 0.9495 ± 0.0146 & 0.9025 ± 0.0036 \\
            \textbf{Multinomial Naive Bayes} & 0.8819 ± 0.0084 & 0.9245 ± 0.0063 & 0.8317 ± 0.0167 & 0.8755 ± 0.0100 \\ \bottomrule
        \end{tabular}
    \end{adjustbox}
    \caption{Results of different ML models on the dataset APT v None APT}
    \label{ml_models_results}
\end{table}


\begin{table}[H]
	\resizebox{\textwidth}{!}{%
		\centering
			\begin{tabular}{|c|c|c|c|c|}
			\hline
            \textbf{Model} & \textbf{Accuracy (Mean ± Std)} & \textbf{Precision (Mean ± Std)} & \textbf{Recall (Mean ± Std)} & \textbf{F1 Score (Mean ± Std)} \\ \hline
            \textbf{Logistic Regression} & 0.9242 ± 0.0104 & 0.9192 ± 0.0074 & 0.9295 ± 0.0159 & 0.9246 ± 0.0107 \\ \hline
            \textbf{Random Forest} & 0.9370 ± 0.0058 & 0.9208 ± 0.0063 & 0.9582 ± 0.0058 & 0.9380 ± 0.0074 \\ \hline
            \textbf{Gradient Boosting} & 0.9448 ± 0.0106 & 0.9413 ± 0.0111 & 0.9483 ± 0.0119 & 0.9441 ± 0.0100 \\ \hline
            \textbf{k-NN} & 0.9152 ± 0.0077 & 0.9128 ± 0.0094 & 0.9183 ± 0.0156 & 0.9154 ± 0.0081 \\ \hline
            \textbf{Gaussian Naive Bayes} & 0.8971 ± 0.0083 & 0.9477 ± 0.0171 & 0.8410 ± 0.0076 & 0.8911 ± 0.0081 \\ \hline
            \textbf{SVC} & 0.9277 ± 0.0089 & 0.9240 ± 0.0014 & 0.9320 ± 0.0177 & 0.9279 ± 0.0094 \\ \hline
            \textbf{AdaBoost} & 0.9314 ± 0.0049 & 0.9246 ± 0.0066 & 0.9395 ± 0.0038 & 0.9320 ± 0.0047 \\ \hline
            \textbf{Linear Discriminant Analysis} & 0.7347 ± 0.0412 & 0.7268 ± 0.0262 & 0.7487 ± 0.0718 & 0.7369 ± 0.0481 \\ \hline
            \textbf{Quadratic Discriminant Analysis} & 0.8974 ± 0.0034 & 0.8602 ± 0.0089 & 0.9495 ± 0.0146 & 0.9025 ± 0.0036 \\ \hline
            \textbf{Multinomial Naive Bayes} & 0.8819 ± 0.0084 & 0.9245 ± 0.0063 & 0.8317 ± 0.0167 & 0.8755 ± 0.0100 \\  \hline
        \end{tabular}
    \end{adjustbox}
    \caption{Results of different ML models on the dataset APT v None APT}
    \label{ml_models_results}
\end{table}

